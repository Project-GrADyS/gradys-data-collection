{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cbc02d-dfe9-4ba1-9425-97cd078069b2",
   "metadata": {},
   "source": [
    "# Second Scenario - Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be78c068-1a51-4dc9-9884-ae0e7872ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d5415-1d75-47f6-b1d4-435b9796fd06",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "The last prototype demonstrated good performance on smaller problems, but its performance was shown to quickly degrade on larger ones. This scenario intends to address this problem. \n",
    "\n",
    "# State\n",
    "Agents know:\n",
    "\n",
    "- The positions of the closest _i_ agents\n",
    "- The positions of the closest _j_ sensors that have not been visited yet\n",
    "- Their own position\n",
    "\n",
    "Now that only _i_ agents and _j_ sensors are visible, the state space no longer linearly scales with the number of drones or sensors, it is constant. Also, since only non-visited sensors are in the state, agents no longer need to know which sensors have been visited.\n",
    "\n",
    "# Reward\n",
    "- Number of sensors visited for the first time in the last iteration\n",
    "- -1 to an agent that leaves the scenario's area\n",
    "\n",
    "This new reward intends to address the sparsity of the previous one. Rewards are more frequent, every time a sensor is visited for the first time instead of once in the end of the episode. It's also more forgiving, as agents are rewarded even if not all sensors are visited by the end of the scenario. This change intends to accelerate learning.\n",
    "\n",
    "# Training\n",
    "While a policy is being learned during training, it is important that a diverse set of experiences are collected, containing both successes and failures. A high quantity of experiences is not enough, they also have to be meaningful. In the previous scenario, if the agents could not visit all sensors and did not leave the simulation area, the episode would end at a parametrized time limit. Since the behavior of not leaving the scenario's area is pretty quickly learned, but the data collection behavior is not, simulations would run until the time limit very frequently. This means that a lot of meaningless data of the agents wondering around in the scenario until the time limit were being collected.\n",
    "\n",
    "To help with this, this termination condition was changed. Instead of terminating the simulation at a parametrized time limit, a stall counter would be implemented. It resets to zero every time a new sensor is visited and counts the number of seconds since a new sensor was last visited. A new parameter was introduced to specify the maximum number of seconds stalled, having been reached the episode is terminated. Using this new strategy, simulations where meaningful data is being collected (sensors are being visited) are allowed to continue, whereas simulations where the agents are \"confused\" can be terminated early. This stall limit can be set much lower than the time limit.\n",
    "\n",
    "# Algorithm\n",
    "The DDPG algorithm makes a training iteration every episode iteration. Since we are using multiple agents, multiple experiences are collected at every iteration. Before we were training once every episode iteration, but we should really be training once for every experience collected, to match the experience x training ratio of the regular DDPG algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469c6b1-2076-4e13-a761-e1d2d47f5504",
   "metadata": {},
   "source": [
    "# Sensor number evaluation\n",
    "To evaluate how these changes have affected results, we will run a campaign evaluating the performance on \n",
    "\n",
    "## Parameters\n",
    "- Number of drones = 1\n",
    "- Number of sensors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- Scenario square size = 100\n",
    "- Training time = 10mil\n",
    "\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59c9dc2a-0bb4-4ab9-8d75-365554c8e0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 58345), started 0:25:33 ago. (Use '!kill 58345' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d167c298f7f91dfc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d167c298f7f91dfc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs/sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939e0b8d-3c92-432a-9675-0c2d871cc4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
