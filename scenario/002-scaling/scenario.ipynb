{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1cbc02d-dfe9-4ba1-9425-97cd078069b2",
   "metadata": {},
   "source": [
    "# Second Scenario - Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be78c068-1a51-4dc9-9884-ae0e7872ddd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69d5415-1d75-47f6-b1d4-435b9796fd06",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "The last prototype demonstrated good performance on smaller problems, but its performance was shown to quickly degrade on larger ones. This scenario intends to address this problem. \n",
    "\n",
    "# State\n",
    "Agents know:\n",
    "\n",
    "- The positions of the closest _i_ agents\n",
    "- The positions of the closest _j_ sensors that have not been visited yet\n",
    "- Their own position\n",
    "\n",
    "Now that only _i_ agents and _j_ sensors are visible, the state space no longer linearly scales with the number of drones or sensors, it is constant. Also, since only non-visited sensors are in the state, agents no longer need to know which sensors have been visited.\n",
    "\n",
    "# Reward\n",
    "- Number of sensors visited for the first time in the last iteration\n",
    "- -1 to an agent that leaves the scenario's area\n",
    "\n",
    "This new reward intends to address the sparsity of the previous one. Rewards are more frequent, every time a sensor is visited for the first time instead of once in the end of the episode. It's also more forgiving, as agents are rewarded even if not all sensors are visited by the end of the scenario. This change intends to accelerate learning.\n",
    "\n",
    "# Training\n",
    "While a policy is being learned during training, it is important that a diverse set of experiences are collected, containing both successes and failures. A high quantity of experiences is not enough, they also have to be meaningful. In the previous scenario, if the agents could not visit all sensors and did not leave the simulation area, the episode would end at a parametrized time limit. Since the behavior of not leaving the scenario's area is pretty quickly learned, but the data collection behavior is not, simulations would run until the time limit very frequently. This means that a lot of meaningless data of the agents wondering around in the scenario until the time limit were being collected.\n",
    "\n",
    "To help with this, this termination condition was changed. Instead of terminating the simulation at a parametrized time limit, a stall counter would be implemented. It resets to zero every time a new sensor is visited and counts the number of seconds since a new sensor was last visited. A new parameter was introduced to specify the maximum number of seconds stalled, having been reached the episode is terminated. Using this new strategy, simulations where meaningful data is being collected (sensors are being visited) are allowed to continue, whereas simulations where the agents are \"confused\" can be terminated early. This stall limit can be set much lower than the time limit.\n",
    "\n",
    "# Algorithm\n",
    "The DDPG algorithm makes a training iteration every episode iteration. Since we are using multiple agents, multiple experiences are collected at every iteration. Before we were training once every episode iteration, but we should really be training once for every experience collected, to match the experience x training ratio of the regular DDPG algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469c6b1-2076-4e13-a761-e1d2d47f5504",
   "metadata": {},
   "source": [
    "# Sensor number evaluation\n",
    "To evaluate how these changes have affected results, we will run a campaign evaluating the performance on \n",
    "\n",
    "## Parameters\n",
    "- Number of drones = 1\n",
    "- Number of sensors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "- Scenario square size = 100\n",
    "- Training time = 10mil\n",
    "\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c9dc2a-0bb4-4ab9-8d75-365554c8e0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-239dcfddeb0689cc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-239dcfddeb0689cc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs/sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f61093-a87f-4cd9-8482-0607ff497cc9",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The optimizations have had an immense impact on performance. All scenarios, even the hardest ones, optimized successfully to some level. The individual impact of each optimization still hast to be measured.\n",
    "\n",
    "The policy converged to a suboptimal solution in some scenarios. Still magnitudes better than previous results, but not quite optimal. Further analysis is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee401c-273f-45a1-975b-992d64be4b97",
   "metadata": {},
   "source": [
    "# Ablation testing\n",
    "We'll be removing individual optimizations to study their impact on performance. Measurements from this experiment evaluates if each of the optimizations implemented in this scenario had an actual impact on results.\n",
    "\n",
    "## Parameters\n",
    "The parameters being varied are:\n",
    "- State: [before optimization (agents know the positions of all agents and sensors), after optimization (agents know the position of the two closest agents (control)]\n",
    "- Reward: [old sparse reward, new dense reward (control)]\n",
    "- Episode length: [old max simulation duration, new max stall duration (control)]\n",
    "- Out of bounds treatment: [punish agents, block agents (control)]\n",
    "- Algorithm training: [train once, train once per agent (control)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afa92415-f136-4f1c-86d6-8cde8c8391ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b820dd888ab8b14a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b820dd888ab8b14a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs/ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889c7a2-13e3-42ab-9257-18ba7db9b6f7",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "Most changes actually made a difference when compared to the control run. The only optimization that didn't seem to effect the results positively was the blockage of out of bounds actions, instead of punishing them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448bf80-bc8d-4f7e-91eb-2753204a1505",
   "metadata": {},
   "source": [
    "# State optimization\n",
    "One possible cause for the suboptimal results obtained in the first campaign in this scenario is the state. It's possible that it is not representing the environment in a way that facilitates learning. Several new states will be developed.\n",
    "\n",
    "## Parameters\n",
    "- States:\n",
    "    - all_positions: Every agent knows the position of all sensors and agents\n",
    "    - absolute: An agent knows the absolute position of himself, the closest _i_ agents and _j_ sensors.\n",
    "    - relative: An agent knows the relative position of the closest _i_ agents and _j_ sensors.\n",
    "    - distance_angle: An agent knows the distance and angle to the closest _i_ agents and _j_ sensors.\n",
    "    - angle: An agent knows the angle to the closest _i_ agents and _j_ sensors.\n",
    "\n",
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e94306-0b09-4403-b4e2-a5cd600f8442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-369cbbd25d02e40c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-369cbbd25d02e40c\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6008;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs/states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2fbbb4-5406-4866-abae-d51bd85c508c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
